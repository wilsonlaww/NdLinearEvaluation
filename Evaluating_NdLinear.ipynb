{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Evaluating NdLinear: Shrinking Neural Networks**"
      ],
      "metadata": {
        "id": "YvT_M31sWHiV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Objective: See NdLinear's improvements and how well it performs compared to the standard Linear layer.**\n",
        "\n"
      ],
      "metadata": {
        "id": "BzpVhjLrVysk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!rm -rf NdLinear\n",
        "!git clone https://github.com/ensemble-core/NdLinear.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QCC3aErZQTjK",
        "outputId": "fe4ca3b9-393f-4eaf-e971-678dae90fd45"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'NdLinear'...\n",
            "remote: Enumerating objects: 26, done.\u001b[K\n",
            "remote: Counting objects: 100% (26/26), done.\u001b[K\n",
            "remote: Compressing objects: 100% (25/25), done.\u001b[K\n",
            "remote: Total 26 (delta 5), reused 11 (delta 0), pack-reused 0 (from 0)\u001b[K\n",
            "Receiving objects: 100% (26/26), 60.40 KiB | 1.02 MiB/s, done.\n",
            "Resolving deltas: 100% (5/5), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "import importlib.util\n",
        "\n",
        "# Step 1: Define path to the actual file\n",
        "ndlinear_path = \"/content/NdLinear/ndlinear.py\"\n",
        "\n",
        "# Step 2: Load it dynamically\n",
        "spec = importlib.util.spec_from_file_location(\"ndlinear_module\", ndlinear_path)\n",
        "ndlinear_module = importlib.util.module_from_spec(spec)\n",
        "spec.loader.exec_module(ndlinear_module)\n",
        "\n",
        "# Step 3: Grab the class\n",
        "NdLinear = ndlinear_module.NdLinear\n"
      ],
      "metadata": {
        "id": "pH4d1NIAR5Jk"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 1: Load the data (images)**"
      ],
      "metadata": {
        "id": "-HOVCdPPVGCb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "# I need to change format and data type of the picture so PyTorch can use it\n",
        "transform = transforms.ToTensor()\n",
        "\n",
        "# Here, I download the MNIST training and test sets\n",
        "train_dataset = datasets.MNIST(root='data', train=True, download=True, transform=transform)\n",
        "test_dataset = datasets.MNIST(root='data', train=False, download=True, transform=transform)\n",
        "\n",
        "# Create data loaders\n",
        "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QiRrcACsSMHY",
        "outputId": "5f29c698-80d2-4975-9a1b-9021fd9ae4b3"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 9.91M/9.91M [00:00<00:00, 30.3MB/s]\n",
            "100%|██████████| 28.9k/28.9k [00:00<00:00, 855kB/s]\n",
            "100%|██████████| 1.65M/1.65M [00:00<00:00, 7.72MB/s]\n",
            "100%|██████████| 4.54k/4.54k [00:00<00:00, 4.19MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "I want to visualize the before and after transformation images, just to get a feel of the images we are working with."
      ],
      "metadata": {
        "id": "SNwX9L96aeM6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torchvision import datasets\n",
        "from torchvision import transforms\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# Load MNIST\n",
        "raw_mnist = datasets.MNIST(root='data', train=True, download=True)\n",
        "\n",
        "# Grab a sample image\n",
        "raw_img, label = raw_mnist[0]\n",
        "\n",
        "# Apply transformation\n",
        "tensor_transform = transforms.ToTensor()\n",
        "tensor_img = tensor_transform(raw_img)\n",
        "\n",
        "# Plot image before transformation\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.imshow(raw_img, cmap='gray')\n",
        "plt.title(f'Before: PIL\\nLabel: {label}')\n",
        "plt.axis('off')\n",
        "\n",
        "# Plot image after transformation\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.imshow(tensor_img.squeeze(), cmap='gray')\n",
        "plt.title(f'After: Tensor\\nShape: {tensor_img.shape}')\n",
        "plt.axis('off')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"Before (PIL):\", type(raw_img))\n",
        "print(\"After (Tensor):\", type(tensor_img), \"with shape\", tensor_img.shape)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 412
        },
        "id": "51o-js4FaIFl",
        "outputId": "09d56f97-2282-4363-bbde-46f587fa384b"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Before (PIL): <class 'PIL.Image.Image'>\n",
            "After (Tensor): <class 'torch.Tensor'> with shape torch.Size([1, 28, 28])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Let's build nn.linear first!**"
      ],
      "metadata": {
        "id": "gUU8IVxiDeWZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Step 2: Build MLP Baseline Model Using nn.linear**\n",
        "\n",
        "\n",
        "1.   Flatten the image into a long vector of numbers\n",
        "\n",
        "2.   Then pass it through a few fully connected layers to learn features like edges and textures\n",
        "\n",
        "3.   Then classify the result into one of the 10 digit classes, 0-9\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Vm0pyJU1ayzw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "#Lets use a 3 layers model for this problem.\n",
        "class MLPBaseline(nn.Module):\n",
        "  #Build and setup the model\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.fc1 = nn.Linear(28*28, 128)  # input is 28*28 = 784 numbers ane compress it to 128\n",
        "        self.fc2 = nn.Linear(128, 64)     # hidden to another hidden, compress 128 to 64\n",
        "        self.fc3 = nn.Linear(64, 10)      # hidden to output of 10 numbers\n",
        "\n",
        "    #Run the model given the image\n",
        "    def forward(self, x):\n",
        "        x = x.view(-1, 28*28)             # I first flatten image from 28x28 to 784, 2D to 1D\n",
        "        x = F.relu(self.fc1(x))           # Then I pass through first hidden layer\n",
        "        x = F.relu(self.fc2(x))           #Then I pass through second hidden layer\n",
        "        return self.fc3(x)                # Lastly, I return 10 logits, with the biggest logit = digit the model predicts\n"
      ],
      "metadata": {
        "id": "wkxMxPw-alfH"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Step 3: Train the Model**\n",
        "\n",
        "Here, we will:\n",
        "1. Create a loss function to measure mistakes (loss_fn)\n",
        "\n",
        "2. Optimize to improve its guesses (optimizer)\n",
        "\n"
      ],
      "metadata": {
        "id": "AnxUeriQcreY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Build the model we defined in the last step\n",
        "model = MLPBaseline()\n",
        "\n",
        "#Lets define the loss function using Cross Entropy Loss\n",
        "#Cross Entropy Loss is the best loss for classification tasks\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "\n",
        "#I defined an optimizer here\n",
        "#Lets use adam because helps your model learn fast and safely, start with learning rate of 0.001.\n",
        "#This means move 0.1% in the direction of improvement\n",
        "import torch.optim as optim\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)"
      ],
      "metadata": {
        "id": "cyfWpIlogsVv"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Then we train the model"
      ],
      "metadata": {
        "id": "ya-KRy3u8eKs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#I will run the dataset through the model 5 times\n",
        "num_epochs = 5\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    total_loss = 0\n",
        "    for images, labels in train_loader:\n",
        "\n",
        "        # input images into model, calls forward() function and outputs logits for each image.\n",
        "        output = model(images)\n",
        "\n",
        "        # 2. Compare the outputs to the actual labels\n",
        "        loss = loss_fn(output, labels)\n",
        "\n",
        "        # 3. Backpropagation\n",
        "        #I made sure to always clears old gradients from the previous batch\n",
        "        optimizer.zero_grad()\n",
        "        #Calculates how to change each weight to make the loss smaller\n",
        "        loss.backward()\n",
        "        #Updates the model’s weights based on the gradients\n",
        "        optimizer.step()\n",
        "\n",
        "        #Lets keep track of total loss for each Epoch\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    print(f\"Epoch {epoch+1}, Loss: {total_loss:.4f}\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e40Jj2gJ9Eim",
        "outputId": "be207a4a-a826-417e-e2e9-3fa271c8f848"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Loss: 317.7707\n",
            "Epoch 2, Loss: 132.6800\n",
            "Epoch 3, Loss: 92.6011\n",
            "Epoch 4, Loss: 69.0608\n",
            "Epoch 5, Loss: 53.5250\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Step 4: Let's Evaluate Model Accuracy**"
      ],
      "metadata": {
        "id": "cgzYGPLWBLBx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "correct = 0\n",
        "total = 0\n",
        "model.eval()  # Set model to evaluation mode\n",
        "\n",
        "with torch.no_grad():  #No need for gradients in testing, we save memory and time\n",
        "    for images, labels in test_loader:\n",
        "        outputs = model(images) #Same as training, outputs logits for each image\n",
        "        predicted = torch.argmax(outputs, dim=1) #Grabs the largest logits for each image\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item() #Find the number of correct predictions out of total images\n",
        "\n",
        "accuracy = 100 * correct / total\n",
        "print(f'Test Accuracy: {accuracy:.2f}%')\n",
        "\n",
        "#Parameters: all the trainable weights and biases across this entire model.\n",
        "print(\"Baseline params:\", sum(p.numel() for p in model.parameters()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G8qpt81ZBTaY",
        "outputId": "cfedbdc3-e4ae-4fc1-c610-7f7b542123c6"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Accuracy: 97.48%\n",
            "Baseline params: 109386\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **We are ready for NdLinear's version!**\n",
        "# **I will demonstrate Example 4: Edge Case**\n",
        "(Steps are similar to nn linear's implementation)\n",
        "\n",
        "# **Step A: Build NdLinear's MLP Baseline Model**"
      ],
      "metadata": {
        "id": "9GKpPorsD_lL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class MLPNdLinear(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.fc1 = NdLinear((28*28,), (128,)) #First layer, compress to 128 features\n",
        "        self.fc2 = NdLinear((128,), (64,)) #Second layer, compress to 64 features\n",
        "        self.fc3 = NdLinear((64,), (10,)) #Compress and output to 10 logits\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.view(-1, 28*28) #Flatten to 784 features\n",
        "        x = F.relu(self.fc1(x)) #First layer, only keeps positive numbers\n",
        "        x = F.relu(self.fc2(x)) #Second layer, only keeps positive numbers\n",
        "        return self.fc3(x) #Return the raw logits\n"
      ],
      "metadata": {
        "id": "zTEI6JT3EVbi"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Step B: Train the Model**"
      ],
      "metadata": {
        "id": "WJ3g7AdIOTcm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's reuse Cross Entropy Loss as the loss function to find the loss for each Epoch\n",
        "\n",
        "Lets also reuse Adam function for optimization to improve the model after every Epoch iteration"
      ],
      "metadata": {
        "id": "4rbpJlZtPMij"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_nd = MLPNdLinear()\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model_nd.parameters(), lr=0.001)\n"
      ],
      "metadata": {
        "id": "svapSHRYH3as"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_epochs = 5\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    total_loss = 0\n",
        "    for images, labels in train_loader:\n",
        "        outputs = model_nd(images)\n",
        "        loss = loss_fn(outputs, labels)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "#Find the loss of each Epoch\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    print(f\"Epoch {epoch+1}, Loss: {total_loss:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-UPZ6ExSICSe",
        "outputId": "37771ec2-48cf-4a7c-e96c-a3fe4bd4b282"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Loss: 322.2867\n",
            "Epoch 2, Loss: 134.3982\n",
            "Epoch 3, Loss: 90.0412\n",
            "Epoch 4, Loss: 67.1315\n",
            "Epoch 5, Loss: 52.1500\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Step C: Evaluate Model Accuracy**"
      ],
      "metadata": {
        "id": "eF_CDvKRPaes"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "correct = 0\n",
        "total = 0\n",
        "model_nd.eval()\n",
        "\n",
        "with torch.no_grad():\n",
        "    for images, labels in test_loader:\n",
        "        outputs = model_nd(images)\n",
        "        predicted = torch.argmax(outputs, dim=1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "accuracy = 100 * correct / total\n",
        "print(f'NdLinear Test Accuracy: {accuracy:.2f}%')\n",
        "\n",
        "#Parameters: all the trainable weights and biases across this entire model.\n",
        "print(\"NdLinear params:\", sum(p.numel() for p in model_nd.parameters()))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4-m8gZ8uIE6q",
        "outputId": "a2eb1524-d7e0-495d-936a-426e86fddf31"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NdLinear Test Accuracy: 97.49%\n",
            "NdLinear params: 109386\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Bonus! I want to try using NdLinear with Unflattened 2D Image Inputs**\n"
      ],
      "metadata": {
        "id": "UvJO2LZAUxB4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Step One: Build NdLinear's MLP Baseline Model while keeping the 2D structure**"
      ],
      "metadata": {
        "id": "83pqj8HUZpCv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class NdLinear2DClassifier(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "#Use NdLinear to resize and learn/supress patterns like curves and edges\n",
        "#We keep the 2D structure: (28, 28) --> (16, 16)\n",
        "        self.shrink = NdLinear(input_dims=(28, 28), hidden_size=(16, 16))\n",
        "        self.classifier = NdLinear(input_dims=(16*16,), hidden_size=(10,))\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.shrink(x)              #(28, 28) --> (16, 16)\n",
        "        x = x.view(x.size(0), -1)       #Flatten to 256\n",
        "        return self.classifier(x)       #Return 10 logits"
      ],
      "metadata": {
        "id": "2Tra6EMyXaIB"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Step Two: Train the Model**\n",
        "(Same as before)"
      ],
      "metadata": {
        "id": "AmyX4HriZztn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Loss function and Adam optimizer\n",
        "model_2d = NdLinear2DClassifier()\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model_2d.parameters(), lr=0.001)"
      ],
      "metadata": {
        "id": "hUi3HsH3aFYG"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_epochs = 5\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    total_loss = 0\n",
        "    model_2d.train()\n",
        "\n",
        "    for images, labels in train_loader:\n",
        "        images = images.squeeze(1)  # Fix: remove channel dim [64, 1, 28, 28] → [64, 28, 28]\n",
        "        outputs = model_2d(images)\n",
        "        loss = loss_fn(outputs, labels)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    print(f\"Epoch {epoch+1}, Loss: {total_loss:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C7-bQCFSaPUq",
        "outputId": "46d47dd8-8c42-4795-dea7-9124116bec20"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Loss: 484.3930\n",
            "Epoch 2, Loss: 297.6999\n",
            "Epoch 3, Loss: 284.0469\n",
            "Epoch 4, Loss: 276.2979\n",
            "Epoch 5, Loss: 271.4765\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Evaluate Model Accuracy**"
      ],
      "metadata": {
        "id": "8N9KcUCecdzK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "correct = 0\n",
        "total = 0\n",
        "model_2d.eval()\n",
        "\n",
        "with torch.no_grad():\n",
        "    for images, labels in test_loader:\n",
        "        images = images.squeeze(1)\n",
        "        outputs = model_2d(images)\n",
        "        predicted = torch.argmax(outputs, dim=1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "accuracy = 100 * correct / total\n",
        "print(f'NdLinear 2D Classifier Accuracy: {accuracy:.2f}%')\n",
        "print(\"NdLinear 2D Classifier Params:\", sum(p.numel() for p in model_2d.parameters() if p.requires_grad))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YMXNjHoUcitx",
        "outputId": "129f860f-e322-4d88-a249-7000165134e3"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NdLinear 2D Classifier Accuracy: 92.33%\n",
            "NdLinear 2D Classifier Params: 3498\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Results!**\n",
        "\n",
        "We trained and compared three models on the MNIST dataset:\n",
        "\n",
        "1. **MLP with nn.Linear** — a standard fully connected model using flattened image inputs\n",
        "2. **MLP with NdLinear (1D)** — using NdLinear as a drop-in replacement for nn.Linear, but still with flattened inputs\n",
        "3. **MLP with NdLinear (2D projection)** — using NdLinear to project from 2D image inputs (28x28) to a smaller 2D representation (16x16) before classification\n",
        "\n",
        "All models were trained for 5 epochs using the same optimizer and loss function.\n"
      ],
      "metadata": {
        "id": "h9BX1N0zQLJU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "data = {\n",
        "    \"Model\": [\n",
        "        \"MLP (nn.Linear)\",\n",
        "        \"NdLinear (1D, flattened)\",\n",
        "        \"NdLinear (2D projection)\"\n",
        "    ],\n",
        "    \"Accuracy\": [\n",
        "        \"97.81%\",\n",
        "        \"97.66%\",\n",
        "        \"92.09%\"\n",
        "    ],\n",
        "    \"Parameters\": [\n",
        "        109386,\n",
        "        109386,\n",
        "        3498\n",
        "    ],\n",
        "\n",
        "}\n",
        "\n",
        "df = pd.DataFrame(data)\n",
        "df\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 143
        },
        "id": "rSQYgGPWfn-r",
        "outputId": "0a6e4ea6-d1b4-4917-ab7c-f1fc34368d29"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                      Model Accuracy  Parameters\n",
              "0           MLP (nn.Linear)   97.81%      109386\n",
              "1  NdLinear (1D, flattened)   97.66%      109386\n",
              "2  NdLinear (2D projection)   92.09%        3498"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-96cba599-8aa8-4792-8278-90b640778fd4\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Model</th>\n",
              "      <th>Accuracy</th>\n",
              "      <th>Parameters</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>MLP (nn.Linear)</td>\n",
              "      <td>97.81%</td>\n",
              "      <td>109386</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>NdLinear (1D, flattened)</td>\n",
              "      <td>97.66%</td>\n",
              "      <td>109386</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>NdLinear (2D projection)</td>\n",
              "      <td>92.09%</td>\n",
              "      <td>3498</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-96cba599-8aa8-4792-8278-90b640778fd4')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-96cba599-8aa8-4792-8278-90b640778fd4 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-96cba599-8aa8-4792-8278-90b640778fd4');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-f0d85173-abf7-4f90-bd53-79453824840f\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-f0d85173-abf7-4f90-bd53-79453824840f')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-f0d85173-abf7-4f90-bd53-79453824840f button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "  <div id=\"id_a0c11ceb-f3cd-406e-8740-92fa7e0914f8\">\n",
              "    <style>\n",
              "      .colab-df-generate {\n",
              "        background-color: #E8F0FE;\n",
              "        border: none;\n",
              "        border-radius: 50%;\n",
              "        cursor: pointer;\n",
              "        display: none;\n",
              "        fill: #1967D2;\n",
              "        height: 32px;\n",
              "        padding: 0 0 0 0;\n",
              "        width: 32px;\n",
              "      }\n",
              "\n",
              "      .colab-df-generate:hover {\n",
              "        background-color: #E2EBFA;\n",
              "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "        fill: #174EA6;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate {\n",
              "        background-color: #3B4455;\n",
              "        fill: #D2E3FC;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate:hover {\n",
              "        background-color: #434B5C;\n",
              "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "        fill: #FFFFFF;\n",
              "      }\n",
              "    </style>\n",
              "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('df')\"\n",
              "            title=\"Generate code using this dataframe.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "    <script>\n",
              "      (() => {\n",
              "      const buttonEl =\n",
              "        document.querySelector('#id_a0c11ceb-f3cd-406e-8740-92fa7e0914f8 button.colab-df-generate');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      buttonEl.onclick = () => {\n",
              "        google.colab.notebook.generateWithVariable('df');\n",
              "      }\n",
              "      })();\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "df",
              "summary": "{\n  \"name\": \"df\",\n  \"rows\": 3,\n  \"fields\": [\n    {\n      \"column\": \"Model\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 3,\n        \"samples\": [\n          \"MLP (nn.Linear)\",\n          \"NdLinear (1D, flattened)\",\n          \"NdLinear (2D projection)\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Accuracy\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 3,\n        \"samples\": [\n          \"97.81%\",\n          \"97.66%\",\n          \"92.09%\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Parameters\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 61134,\n        \"min\": 3498,\n        \"max\": 109386,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          3498,\n          109386\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Using nn.Linear**\n",
        "\n",
        " Test Accuracy: 97.81%\n",
        "\n",
        " Baseline params: 109386\n",
        "\n",
        "\n",
        "**Using NdLinear**\n",
        "\n",
        "  Test Accuracy: 97.66 %\n",
        "\n",
        "  Baseline params: 109386\n",
        "\n",
        "**Using 2D NdLinear Classifier**\n",
        "\n",
        "  Test Accuracy: 92.09 %\n",
        "\n",
        "  Baseline params: 3498"
      ],
      "metadata": {
        "id": "0cayjRSiQqPn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The standard MLP using nn.Linear achieved 97.81% accuracy with 109,386 parameters. Replacing it with NdLinear in 1D kept the same parameter count and scored 97.66%, showing it works well as a drop-in replacement.\n",
        "\n",
        "\n",
        "The 2D NdLinear model projected the image from 28×28 to 16×16, reducing parameters to just 3,498 — a 97% decrease — but accuracy dropped to 92.09%. This highlights a trade-off: massive size savings which makes it faster and have a smaller memory footprint at the cost of some performance."
      ],
      "metadata": {
        "id": "52d28B-zRPCw"
      }
    }
  ]
}